{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iD3IaYrW3903"
      },
      "source": [
        "#Pretraining LLMs\n",
        "## Description\n",
        "Having processed our text we will be start by implmenting the training loop and code out a basic model evaluation to pretrain an LLM\n",
        "\n",
        "Prereqs before the lab: Build a Large Language Model (From Scratch) ch4 (video or textformat)\n",
        "\n",
        "Citation\n",
        "Raschka, S. (2024). Build a large language model (from scratch). Manning Publications.\n",
        "\n",
        "Lab Deliverables\n",
        "Read though: https://github.com/rasbt/LLM-workshop-2024/blob/main/04_pretraining/04.ipynb\n",
        "\n",
        "Set up\n",
        "Be sure to upload the supplementry file and frankenstineClean.txt provided in the assigment page on canvas.\n",
        "\n",
        "\n",
        "## Learning Goals\n",
        "\n",
        "\n",
        "This packages we will start with for this one will be matplotlib, numpy, ticktoken and torch for pytorch\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22esN5KI5X4n"
      },
      "source": [
        "#Step 1. Use GPT to generate Text\n",
        "1. Initalize the GPT model we initalized from last Lab.\n",
        "<details>\n",
        "  <summary>Click Here to view solution</summary>\n",
        "    ```python\n",
        "    import torch\n",
        "    from supplementary import GPTModel\n",
        "\n",
        "\n",
        "    GPT_CONFIG_124M = {\n",
        "        \"vocab_size\": 50257,   # Vocabulary size\n",
        "        \"context_length\": 256, # Shortened context length (orig: 1024)\n",
        "        \"emb_dim\": 768,        # Embedding dimension\n",
        "        \"n_heads\": 12,         # Number of attention heads\n",
        "        \"n_layers\": 12,        # Number of layers\n",
        "        \"drop_rate\": 0.1,      # Dropout rate\n",
        "        \"qkv_bias\": False      # Query-key-value bias\n",
        "    }\n",
        "\n",
        "    torch.manual_seed(123)\n",
        "    model = GPTModel(GPT_CONFIG_124M)\n",
        "    model.eval();  # Disable dropout during inference\n",
        "    ```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cWDDNetL6bvC"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GPTModel(\n",
              "  (tok_emb): Embedding(50257, 768)\n",
              "  (pos_emb): Embedding(256, 768)\n",
              "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
              "  (trf_blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (4): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (5): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (6): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (7): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (8): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (9): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (10): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (11): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (final_norm): LayerNorm()\n",
              "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "from supplementary import GPTModel\n",
        "\n",
        "# same config from last week\n",
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,\n",
        "    \"context_length\": 256, \n",
        "    \"emb_dim\": 768,\n",
        "    \"n_heads\": 12,\n",
        "    \"n_layers\": 12,\n",
        "    \"drop_rate\": 0.1,\n",
        "    \"qkv_bias\": False\n",
        "}\n",
        "\n",
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.eval() "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pQ9eWDS7Ave"
      },
      "source": [
        "# Step 2. Generate Text pt 2\n",
        "In the previous lab, we created generate_text_simple. We will use two of its functions for encoding and decoding our tokens. We will also create some helper functions to make the conversion a bit easier.\n",
        "\n",
        "1. Use the generate_text_simple function from the previous labs. A version of it is in the supplementary file, so it can be imported and used.\n",
        "2. Create a function called text_to_token_ids\n",
        "  - Takes text and tokenizer as parameters\n",
        "  - Encodes the text and saves it to a variable\n",
        "  - Calls torch.tensor on the encoded text, then chains .unsqueeze(0) at the end\n",
        "  - Returns the encoded_tensor\n",
        "3. Create a function called token_ids_to_text\n",
        "  - Takes token_ids and tokenizer as parameters\n",
        "  - Calls token_ids.squeeze(0) and saves it to a variable called flat\n",
        "  - Returns tokenizer.decode(flat.tolist())\n",
        "4. Create a string and save it to start_context. Initialize the tokenizer with\n",
        "tiktoken.get_encoding(\"gpt2\").\n",
        "5. Call generate_text_simple and pass in:\n",
        "  - model=model\n",
        "  - idx=text_to_token_ids(start_context, tokenizer)\n",
        "  - max_new_tokens=10\n",
        "  - context_size=GPT_CONFIG_124M[\"context_length\"]\n",
        "6. Print the result of token_ids_to_text when passed token_ids and tokenizer. (Note that the results here won't be great yet because the model hasn't been trained yet)\n",
        "<details>\n",
        "  <summary>Click Here to view solution</summary>\n",
        "    ```python\n",
        "    import tiktoken\n",
        "    from supplementary import generate_text_simple\n",
        "\n",
        "\n",
        "    def text_to_token_ids(text, tokenizer):\n",
        "        encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
        "        encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
        "        return encoded_tensor\n",
        "\n",
        "    def token_ids_to_text(token_ids, tokenizer):\n",
        "        flat = token_ids.squeeze(0) # remove batch dimension\n",
        "        return tokenizer.decode(flat.tolist())\n",
        "    start_context = \"Every effort moves you\"\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    token_ids = generate_text_simple(\n",
        "        model=model,\n",
        "        idx=text_to_token_ids(start_context, tokenizer),\n",
        "        max_new_tokens=10,\n",
        "        context_size=GPT_CONFIG_124M[\"context_length\"]\n",
        "    )\n",
        "\n",
        "    print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))\n",
        "\n",
        "    #New cell\n",
        "    start_context = \"Every effort moves you\"\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    token_ids = generate_text_simple(\n",
        "        model=model,\n",
        "        idx=text_to_token_ids(start_context, tokenizer),\n",
        "        max_new_tokens=10,\n",
        "        context_size=GPT_CONFIG_124M[\"context_length\"]\n",
        "    )\n",
        "\n",
        "    print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))\n",
        "    ```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fBOA-z2u977V"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output text:\n",
            " Every effort moves you rentingetic wasnم refres RexMeCHicular stren\n"
          ]
        }
      ],
      "source": [
        "import tiktoken\n",
        "from supplementary import generate_text_simple\n",
        "\n",
        "\n",
        "def text_to_token_ids(text, tokenizer):\n",
        "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
        "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
        "    return encoded_tensor\n",
        "\n",
        "\n",
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "    flat = token_ids.squeeze(0) \n",
        "    return tokenizer.decode(flat.tolist())\n",
        "\n",
        "start_context = \"Every effort moves you\"\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "token_ids = generate_text_simple(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(start_context, tokenizer),\n",
        "    max_new_tokens=10,\n",
        "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
        ")\n",
        "\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbL2A4Tk9_Fw",
        "outputId": "4fe9c8ec-2ccb-437f-b5b5-abec9aec9c74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output text:\n",
            " Every effort moves you rentingetic wasnم refres RexMeCHicular stren\n"
          ]
        }
      ],
      "source": [
        "# the output is garbage right now because the model hasnt been trained yet\n",
        "# just want to make sure everything is hooked up correctly\n",
        "start_context = \"Every effort moves you\"\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "token_ids = generate_text_simple(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(start_context, tokenizer),\n",
        "    max_new_tokens=10,\n",
        "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
        ")\n",
        "\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyFG0qed-bj_"
      },
      "source": [
        "# Step 3. Preparing the dataset loaders\n",
        "We will be using a few chapters of frankenstine so the training completels faster, if you'd like to train it on more text feel free to but note that it will take a while to process.\n",
        "1. Read in frankenstine.txt\n",
        "2. Verify the text loaded by printing the first and last 100 words\n",
        "3. Create a variable called total_charactes and set it to len(text_data)\n",
        "4. create total_tokens and set it to len(tokenizer.encode(text_data)\n",
        "5. print the variables from 3 and 4\n",
        "\n",
        "<details>\n",
        "  <summary>Click Here to view solution</summary>\n",
        "    ```python\n",
        "    with open(\"frankenstine.txt\", \"r\", encoding=\"utf-8\") as file:\n",
        "    text_data = file.read()\n",
        "\n",
        "    # New Cell\n",
        "    print(text_data[:99])\n",
        "\n",
        "    # New Cell\n",
        "    print(text_data[-99:])\n",
        "\n",
        "    # New Cell\n",
        "    total_characters = len(text_data)\n",
        "    total_tokens = len(tokenizer.encode(text_data))\n",
        "\n",
        "    print(\"Characters:\", total_characters)\n",
        "    print(\"Tokens:\", total_tokens)\n",
        "    ```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "eoVWtvIKUFYv"
      },
      "outputs": [],
      "source": [
        "with open(\"frankenstine.txt\", \"r\", encoding=\"utf-8\") as file:\n",
        "    text_data = file.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5sKDULetUi6y",
        "outputId": "960d3aa6-96af-4760-c79f-5073362de14a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chapter 1\n",
            "\n",
            "\n",
            "I am by birth a Genevese, and my family is one of the most\n",
            "distinguished of that republ\n"
          ]
        }
      ],
      "source": [
        "# checking the start of the file\n",
        "print(text_data[:99])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-7F04vRUktB",
        "outputId": "3ae3b419-2904-4629-e8e9-3a3498d36eef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " requested, and I took my leave.\n",
            "\n",
            "Thus ended a day memorable to me; it decided my future destiny.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# checking the end of the file\n",
        "print(text_data[-99:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3y_JQtkmUmV9",
        "outputId": "7e8e295a-fda4-4d76-a359-40ad0be1455e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Characters: 38143\n",
            "Tokens: 8904\n"
          ]
        }
      ],
      "source": [
        "total_characters = len(text_data)\n",
        "total_tokens = len(tokenizer.encode(text_data))\n",
        "\n",
        "print(\"Characters:\", total_characters)\n",
        "print(\"Tokens:\", total_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-kNHP6NU0ys"
      },
      "source": [
        "# Step 4. Data preperation\n",
        "1. import create_dataloader_v1 from supplementary\n",
        "2. Create our training and validaiton variables.\n",
        "- train_ratio = 0.90\n",
        "- total_characters = len(text_data)\n",
        "- total_tokens = len(tokenizer.encode(text_data))\n",
        "- val_data = text_data[split_idx:]\n",
        "3. call toruch.manual_seed and pass in 123\n",
        "4. call create_dataloader_v1 and set it to the variable train_loader with the following arguments\n",
        "  - train_data,\n",
        "  - batch_size=2,\n",
        "  - max_length=GPT_CONFIG_124M[\"context_length\"],\n",
        "  - stride=GPT_CONFIG_124M[\"context_length\"],\n",
        "  - drop_last=True,\n",
        "  - shuffle=True,\n",
        "  - num_workers=0\n",
        "5. call it again but this time set it to a variable called val_loaded with the following arguments\n",
        "  - val_data,\n",
        "  - batch_size=2,\n",
        "  - max_length=GPT_CONFIG_124M[\"context_length\"],\n",
        "  - stride=GPT_CONFIG_124M[\"context_length\"],\n",
        "  - drop_last=False,\n",
        "  - shuffle=False,\n",
        "  - num_workers=0\n",
        "6. Check that data was loaded correctly with prints\n",
        "<details>\n",
        "  <summary>Click Here to view solution</summary>\n",
        "    ```python\n",
        "    from supplementary import create_dataloader_v1\n",
        "\n",
        "\n",
        "    # Train/validation ratio\n",
        "    train_ratio = 0.90\n",
        "    split_idx = int(train_ratio * len(text_data))\n",
        "    train_data = text_data[:split_idx]\n",
        "    val_data = text_data[split_idx:]\n",
        "\n",
        "\n",
        "    torch.manual_seed(123)\n",
        "\n",
        "    train_loader = create_dataloader_v1(\n",
        "        train_data,\n",
        "        batch_size=2,\n",
        "        max_length=GPT_CONFIG_124M[\"context_length\"],\n",
        "        stride=GPT_CONFIG_124M[\"context_length\"],\n",
        "        drop_last=True,\n",
        "        shuffle=True,\n",
        "        num_workers=0\n",
        "    )\n",
        "\n",
        "    val_loader = create_dataloader_v1(\n",
        "        val_data,\n",
        "        batch_size=2,\n",
        "        max_length=GPT_CONFIG_124M[\"context_length\"],\n",
        "        stride=GPT_CONFIG_124M[\"context_length\"],\n",
        "        drop_last=False,\n",
        "        shuffle=False,\n",
        "        num_workers=0\n",
        "    )\n",
        "\n",
        "    #New Cell\n",
        "    print(\"Train loader:\")\n",
        "    for x, y in train_loader:\n",
        "        print(x.shape, y.shape)\n",
        "\n",
        "    print(\"\\nValidation loader:\")\n",
        "    for x, y in val_loader:\n",
        "        print(x.shape, y.shape)\n",
        "    ```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ZdfwWGthWgWn"
      },
      "outputs": [],
      "source": [
        "from supplementary import create_dataloader_v1\n",
        "\n",
        "# splitting 90% for training and 10% for validation\n",
        "train_ratio = 0.90\n",
        "split_idx = int(train_ratio * len(text_data))\n",
        "train_data = text_data[:split_idx]\n",
        "val_data = text_data[split_idx:]\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "train_loader = create_dataloader_v1(\n",
        "    train_data,\n",
        "    batch_size=2,\n",
        "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
        "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
        "    drop_last=True,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "val_loader = create_dataloader_v1(\n",
        "    val_data,\n",
        "    batch_size=2,\n",
        "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
        "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
        "    drop_last=False,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCIQF7WLWxI-",
        "outputId": "fd305dc1-9458-4bf6-a089-34843554ebc1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train loader:\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "\n",
            "Validation loader:\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([1, 256]) torch.Size([1, 256])\n"
          ]
        }
      ],
      "source": [
        "# sanity check to make sure the shapes look right\n",
        "print(\"Train loader:\")\n",
        "for x, y in train_loader:\n",
        "    print(x.shape, y.shape)\n",
        "\n",
        "print(\"\\nValidation loader:\")\n",
        "for x, y in val_loader:\n",
        "    print(x.shape, y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4b3Fw1iW04U",
        "outputId": "fc68543e-a844-43cb-a71f-dab5f13f31f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train batches: 15\n",
            "Val batches: 2\n"
          ]
        }
      ],
      "source": [
        "print(\"Train batches:\", len(train_loader))\n",
        "print(\"Val batches:\", len(val_loader))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2R1FKTwW-F5"
      },
      "source": [
        "# Step 5. Loss calculation\n",
        "We will need to calculate the inital loss before training. This will take a few minutes.\n",
        "1. import calc_loss_loader\n",
        "2. create a variable called device and set it to toruch.device with the argument \"cpu\"\n",
        "3. assigne the model to the divice by calling model.to and passing the divice variable\n",
        "4. call toruch.manual_seed and pass it 123\n",
        "5. Disable the gradent tracking by creating a with block and calling torch.no_grad() and then cal cal_loss_loader for the trining and validation.\n",
        "```\n",
        "with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
        "    train_loss = calc_loss_loader(train_loader, model, device)\n",
        "    val_loss = calc_loss_loader(val_loader, model, device)\n",
        "```\n",
        "5. print out the raining and validation loss\n",
        "<details>\n",
        "  <summary>Click Here to view solution</summary>\n",
        "    ```python\n",
        "    from supplementary import calc_loss_loader\n",
        "    torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader\n",
        "    device = torch.device(\"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
        "        train_loss = calc_loss_loader(train_loader, model, device)\n",
        "        val_loss = calc_loss_loader(val_loader, model, device)\n",
        "\n",
        "    print(\"Training loss:\", train_loss)\n",
        "    print(\"Validation loss:\", val_loss)\n",
        "  ```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1J13ci22XOma",
        "outputId": "d1bfc0c5-4f5f-4924-b442-20711225e30b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss: 10.987987645467122\n",
            "Validation loss: 10.986222743988037\n"
          ]
        }
      ],
      "source": [
        "from supplementary import calc_loss_loader\n",
        "\n",
        "device = torch.device(\"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "with torch.no_grad():\n",
        "    train_loss = calc_loss_loader(train_loader, model, device)\n",
        "    val_loss = calc_loss_loader(val_loader, model, device)\n",
        "\n",
        "print(\"Training loss:\", train_loss)\n",
        "print(\"Validation loss:\", val_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUr14YBUZU-x"
      },
      "source": [
        "# Step 6. Train the LLM\n",
        "Time to train the LLM. The training loop for this is a bit intense, so don’t be afraid to look at the solution for this one. Once you start training, it’s time for a break. Go get a snack, because this is going to take a bit.\n",
        "\n",
        "1. import from supplementary,    \n",
        "  - calc_loss_batch,\n",
        "  - evaluate_model,\n",
        "  - generate_and_print_sample\n",
        "\n",
        "2. Create a funciton called train_model_simple and pass it the following arguments\n",
        "- model\n",
        "- train_loader\n",
        "- val_loader\n",
        "- optimizer\n",
        "- device\n",
        "- num_epochs\n",
        "- eval_freq\n",
        "- eval_iter\n",
        "- start_context\n",
        "- tokenizer\n",
        "3. In the funciton create 3 Lists and set them to the variables train_losses, val_losses, track_tokens_seen. set token_seen to 0 and global_step to -1\n",
        "4. Create a loop `for epoch in range(num_epochs)` and call model.train()\n",
        "5. Also within that loop create another loop `for input_batch, target_batch in train_loader:` and call the following\n",
        "  -   optimizer.zero_grad()\n",
        "  - loss = calc_loss_batch(input_batch,target_batch, model, device)\n",
        "  - loss.backward()\n",
        "  - optimizer.step()\n",
        "  - tokens_seen += input_batch.numel()\n",
        "  - global_step += 1\n",
        "  - if global_step % eval_freq == 0:\n",
        "                - train_loss, val_loss = evaluate_model(\n",
        "                    model, train_loader, val_loader, device, eval_iter)\n",
        "                - train_losses.append(train_loss)\n",
        "                - val_losses.append(val_loss)\n",
        "                - track_tokens_seen.append(tokens_seen)\n",
        "                - print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
        "\n",
        "6. Now out of the nested loop but still in the first loop call generate_and_print_sample and pass it model, tokenizer, device, start_context.\n",
        "7. Return train_loss, val_losses and track_tokens_seen from the function (note this will be out of the loop)\n",
        "\n",
        "8. call torch.manual_seed and pass it 123, Set the model variable to GPTModel and pass i tGPT_CONFIG_124M. Call model.to and pass it device.\n",
        "9. create the optomizer variable by calling torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
        "10. set the number of epochs, I recomend 10\n",
        "11. Lastly train the model\n",
        "```\n",
        "train_losses, val_losses, tokens_seen = train_model_simple(\n",
        "    model, train_loader, val_loader, optimizer, device,\n",
        "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
        "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
        ")\n",
        "```\n",
        "\n",
        "<details>\n",
        "  <summary>Click Here to view solution</summary>\n",
        "    ```python\n",
        "    from supplementary import (\n",
        "    calc_loss_batch,\n",
        "    evaluate_model,\n",
        "    generate_and_print_sample\n",
        ")\n",
        "\n",
        "\n",
        "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
        "                       eval_freq, eval_iter, start_context, tokenizer):\n",
        "    # Initialize lists to track losses and tokens seen\n",
        "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
        "    tokens_seen, global_step = 0, -1\n",
        "\n",
        "    # Main training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "        \n",
        "        for input_batch, target_batch in train_loader:\n",
        "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            loss.backward() # Calculate loss gradients\n",
        "            optimizer.step() # Update model weights using loss gradients\n",
        "            tokens_seen += input_batch.numel()\n",
        "            global_step += 1\n",
        "\n",
        "            # Optional evaluation step\n",
        "            if global_step % eval_freq == 0:\n",
        "                train_loss, val_loss = evaluate_model(\n",
        "                    model, train_loader, val_loader, device, eval_iter)\n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "                track_tokens_seen.append(tokens_seen)\n",
        "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
        "\n",
        "        # Print a sample text after each epoch\n",
        "        generate_and_print_sample(\n",
        "            model, tokenizer, device, start_context\n",
        "        )\n",
        "        return train_losses, val_losses, track_tokens_seen\n",
        "        torch.manual_seed(123)\n",
        "\n",
        "    torch.manual_seed(123)\n",
        "    model = GPTModel(GPT_CONFIG_124M)\n",
        "    model.to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
        "\n",
        "    num_epochs = 10\n",
        "    train_losses, val_losses, tokens_seen = train_model_simple(\n",
        "        model, train_loader, val_loader, optimizer, device,\n",
        "        num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
        "        start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
        "    )\n",
        "    ```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "0yleO8OMZnOV"
      },
      "outputs": [],
      "source": [
        "from supplementary import (\n",
        "    calc_loss_batch,\n",
        "    evaluate_model,\n",
        "    generate_and_print_sample\n",
        ")\n",
        "\n",
        "\n",
        "def train_model_simple(model, train_loader, val_loader, optimizer, device,\n",
        "                       num_epochs, eval_freq, eval_iter, start_context, tokenizer):\n",
        "\n",
        "    # keeping track of things as we train\n",
        "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
        "    tokens_seen = 0\n",
        "    global_step = -1\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "\n",
        "        for input_batch, target_batch in train_loader:\n",
        "            optimizer.zero_grad() # clear out old gradients\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            loss.backward() # backprop\n",
        "            optimizer.step() # update weights\n",
        "            tokens_seen += input_batch.numel()\n",
        "            global_step += 1\n",
        "\n",
        "            # print out loss every eval_freq steps\n",
        "            if global_step % eval_freq == 0:\n",
        "                train_loss, val_loss = evaluate_model(\n",
        "                    model, train_loader, val_loader, device, eval_iter\n",
        "                )\n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "                track_tokens_seen.append(tokens_seen)\n",
        "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
        "\n",
        "        # after each epoch show a sample of what the model is generating\n",
        "        generate_and_print_sample(model, tokenizer, device, start_context)\n",
        "\n",
        "    return train_losses, val_losses, track_tokens_seen\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3rL5zCpZr4G",
        "outputId": "d42d7ce7-9063-45e5-a5fe-8c4bd66d8dba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ep 1 (Step 000000): Train loss 9.810, Val loss 9.854\n",
            "Ep 1 (Step 000005): Train loss 8.124, Val loss 8.359\n",
            "Ep 1 (Step 000010): Train loss 7.041, Val loss 7.546\n",
            "Every effort moves you                                                  \n",
            "Ep 2 (Step 000015): Train loss 6.566, Val loss 7.312\n",
            "Ep 2 (Step 000020): Train loss 6.357, Val loss 7.327\n",
            "Ep 2 (Step 000025): Train loss 6.261, Val loss 7.384\n",
            "Every effort moves you, and                                      , and, and,, and, and,\n",
            "Ep 3 (Step 000030): Train loss 6.196, Val loss 7.418\n",
            "Ep 3 (Step 000035): Train loss 6.213, Val loss 7.444\n",
            "Ep 3 (Step 000040): Train loss 6.158, Val loss 7.427\n",
            "Every effort moves you the the the the of the the of the the of the the of the the of the the of the the of the   the of the the of the  the of the the of\n",
            "Ep 4 (Step 000045): Train loss 5.947, Val loss 7.338\n",
            "Ep 4 (Step 000050): Train loss 6.114, Val loss 7.439\n",
            "Ep 4 (Step 000055): Train loss 6.011, Val loss 7.408\n",
            "Every effort moves you, and my      of, and the of, and my   the the the, and my  the, and my of, and my the, and my the, and my \n",
            "Ep 5 (Step 000060): Train loss 5.844, Val loss 7.267\n",
            "Ep 5 (Step 000065): Train loss 5.747, Val loss 7.300\n",
            "Ep 5 (Step 000070): Train loss 5.617, Val loss 7.374\n",
            "Every effort moves you, and                          the the     the, and the the, and the the, and the \n",
            "Ep 6 (Step 000075): Train loss 5.361, Val loss 7.251\n",
            "Ep 6 (Step 000080): Train loss 5.119, Val loss 7.185\n",
            "Ep 6 (Step 000085): Train loss 5.107, Val loss 7.246\n",
            "Every effort moves you  of the    of of the greatest of the of, and the  the to me.  of. of of the  of of the the, and the the of the of of\n",
            "Ep 7 (Step 000090): Train loss 4.430, Val loss 7.130\n",
            "Ep 7 (Step 000095): Train loss 4.180, Val loss 7.105\n",
            "Ep 7 (Step 000100): Train loss 4.117, Val loss 7.071\n",
            "Every effort moves you the  the of my mother was the greatest was to her to the greatest of the greatest of the greatest.               the, and the greatest of the greatest, and the greatest\n",
            "Ep 8 (Step 000105): Train loss 3.972, Val loss 7.106\n",
            "Ep 8 (Step 000110): Train loss 3.394, Val loss 7.103\n",
            "Ep 8 (Step 000115): Train loss 3.214, Val loss 7.132\n",
            "Every effort moves you the and the the, and the greatest of the the of the  the and of the the, and the and the spirit of the   of, and in the the, and in the the and\n",
            "Ep 9 (Step 000120): Train loss 3.164, Val loss 7.161\n",
            "Ep 9 (Step 000125): Train loss 2.923, Val loss 7.166\n",
            "Ep 9 (Step 000130): Train loss 2.637, Val loss 7.250\n",
            "Every effort moves you of the the explanation of the most of the of of the   of of my father. He was a of tenderness. He had been  of of the the world. He had been had been. He\n",
            "Ep 10 (Step 000135): Train loss 2.258, Val loss 7.229\n",
            "Ep 10 (Step 000140): Train loss 2.181, Val loss 7.276\n",
            "Ep 10 (Step 000145): Train loss 1.763, Val loss 7.299\n",
            "Every effort moves you the and the daughter of a  dismissed me, the most of science, and that rude hand. was deeply grieved by his retreat in these unfortunate circumstances. sister, and the false pride which led to\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.to(device)\n",
        "\n",
        "# AdamW is standard for transformers, lr and weight_decay from the book\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "train_losses, val_losses, tokens_seen = train_model_simple(\n",
        "    model, train_loader, val_loader, optimizer, device,\n",
        "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
        "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvz3t3_2edFn"
      },
      "source": [
        "# Step 7. Save the model and plot the loss\n",
        "1. call torch.save and pass it the model.state_dict() and \"model.pth\n",
        "2. import plot_losses from supplementary\n",
        "3. call torch.linspace set to the variable epochs_tensor and pass it 0, num_epochs, len(train_losses)\n",
        "4. call plot_losses and pass it epochs_tensor, tokens_seen, train_loss and val_losses\n",
        "<details>\n",
        "  <summary>Click Here to view solution</summary>\n",
        "    ```python\n",
        "    torch.save(model.state_dict(), \"model.pth\")\n",
        "    \n",
        "    from supplementary import plot_losses\n",
        "\n",
        "\n",
        "    epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "    plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)\n",
        "    ```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "mAWL_cY1fRRW",
        "outputId": "ef91800a-401f-4c72-dc91-5c5bd4c21fe6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model saved\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 500x300 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "torch.save(model.state_dict(), \"model.pth\")\n",
        "print(\"model saved\")\n",
        "\n",
        "from supplementary import plot_losses\n",
        "\n",
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJkuqQG_fInG"
      },
      "source": [
        "Exercise 1: Generate text from the pretrained LLM\n",
        "Use the model to generate new text (HINT: scroll up to see how we generated text before)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Exercise 2: Load the pretrained model in a new session\n",
        "Open a new Python session or Jupyter notebook and load the model there"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Every effort moves you\n",
            "the and the daughter of a\n",
            "\n",
            "dismissed me, the most\n",
            "of science, and that rude hand.\n",
            "was deeply grieved by his retreat in these unfortunate circumstances.\n",
            "sister, and the false pride which led to\n"
          ]
        }
      ],
      "source": [
        "# Exercise 1 - generate text from the trained model\n",
        "# model should produce much better output now compared to step 2\n",
        "\n",
        "model.eval()\n",
        "\n",
        "start_context = \"Every effort moves you\"\n",
        "\n",
        "token_ids = generate_text_simple(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(start_context, tokenizer),\n",
        "    max_new_tokens=50,\n",
        "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
        ")\n",
        "\n",
        "print(token_ids_to_text(token_ids, tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\STA\\AppData\\Local\\Temp\\ipykernel_3644\\1596179488.py:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model_loaded.load_state_dict(torch.load(\"model.pth\"))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Every effort moves you\n",
            "the and the daughter of a\n",
            "\n",
            "dismissed me, the most\n",
            "of science, and that rude hand.\n",
            "was deeply grieved by his retreat in these unfortunate circumstances.\n",
            "sister, and the false pride which led to\n"
          ]
        }
      ],
      "source": [
        "# Exercise 2 - loading the model back in from the saved file\n",
        "# this simulates opening a fresh session and picking up where we left off\n",
        "\n",
        "import torch\n",
        "import tiktoken\n",
        "from supplementary import GPTModel, generate_text_simple\n",
        "\n",
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,\n",
        "    \"context_length\": 256,\n",
        "    \"emb_dim\": 768,\n",
        "    \"n_heads\": 12,\n",
        "    \"n_layers\": 12,\n",
        "    \"drop_rate\": 0.1,\n",
        "    \"qkv_bias\": False\n",
        "}\n",
        "\n",
        "# build a fresh model with the same config\n",
        "model_loaded = GPTModel(GPT_CONFIG_124M)\n",
        "\n",
        "# load the weights we saved earlier\n",
        "model_loaded.load_state_dict(torch.load(\"model.pth\"))\n",
        "model_loaded.eval()\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "def text_to_token_ids(text, tokenizer):\n",
        "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
        "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
        "    return encoded_tensor\n",
        "\n",
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "    flat = token_ids.squeeze(0)\n",
        "    return tokenizer.decode(flat.tolist())\n",
        "\n",
        "start_context = \"Every effort moves you\"\n",
        "\n",
        "token_ids = generate_text_simple(\n",
        "    model=model_loaded,\n",
        "    idx=text_to_token_ids(start_context, tokenizer),\n",
        "    max_new_tokens=50,\n",
        "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
        ")\n",
        "\n",
        "print(token_ids_to_text(token_ids, tokenizer))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "school_3_10",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
